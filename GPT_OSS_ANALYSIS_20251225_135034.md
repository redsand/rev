# gpt-oss Model Analysis - Performance Issues

**Date**: 2025-12-25
**Log File**: `rev_run_20251225_135034.log` (2623 lines)
**Model**: gpt-oss:120b-cloud

## Executive Summary

The gpt-oss model is struggling with three critical issues:

1. ✗ **Wrong Tool Selection** - CodeWriterAgent calling `read_file` for ADD tasks instead of `write_file` (21 occurrences)
2. ✗ **Planner Output Format** - Planner outputting malformed responses without action brackets (15 occurrences)
3. ✗ **Model-Specific Tokens** - gpt-oss adding `<|message|>` tokens in responses, breaking parsing

### Impact Stats
- **28 failed tasks** / **21 completed tasks** = **57% failure rate**
- **21** "Write action completed without write tool execution" errors
- **15** "No action brackets found" errors
- **Execution still running after 15+ minutes** with minimal progress

---

## Issue 1: CodeWriterAgent Calling Wrong Tools for ADD Tasks

### The Problem

**Task**: action_type="add", description="create tests/user.test.js with Jest/Supertest tests..."
**Expected Tool**: `write_file`
**Actual Tool Called**: `read_file` (line 230) or `run_all_analysis` (lines 2442, 2516)

**Example from log** (lines 210-234):
```
CodeWriterAgent executing task: create tests/user.test.js with Jest/Supertest tests for user registration...
  -> CodeWriterAgent will call tool 'read_file'
  ⏳ Applying read_file to app.js...
  ✓ Successfully applied read_file
  ✗ [FAILED] | Reason: Write action completed without write tool execution
```

**This happened 21 times** throughout the run.

### Root Cause

**File**: `rev/agents/code_writer.py:599-601`

The code correctly limits candidate tools:
```python
elif task.action_type == "add":
    # File creation tasks only get write_file tool
    tool_names = ['write_file']
```

**However**, the `build_context_and_tools()` function (rev/agents/context_provider.py:134-176) uses semantic retrieval to select tools:

```python
bundle = builder.build(
    query=query,
    tool_universe=tool_universe,
    tool_candidates=candidate_tool_names,  # ['write_file']
    top_k_tools=max_tools,
)
selected_tool_schemas = [t.schema for t in bundle.selected_tool_schemas]
```

**The Problem**: The `builder.build()` method can return tools that are **NOT** in `candidate_tool_names`. The semantic search adds tools like `read_file` and `run_all_analysis` based on query similarity, ignoring the strict candidate list.

**Missing Validation**: There's no filtering step that enforces:
```python
# MISSING: Filter to only allowed tools
selected_tool_schemas = [t for t in selected_tool_schemas
                        if t.get("function", {}).get("name") in candidate_tool_names]
```

### Impact

1. **ADD tasks fail immediately** - Can't create files because `write_file` tool not used
2. **Repeated failures** - Each retry gets the same wrong tools
3. **Context pollution** - Failed tasks fill context with errors
4. **No forward progress** - System stuck in retry loops

---

## Issue 2: Planner Output Format Problems

### The Problem

The planner (Orchestrator deciding next tasks) is outputting malformed responses **15 times**.

**Expected format**:
```
[ACTION_TYPE] description of task
```

**Actual gpt-oss output** (line 82, 2467, 2608):
```
"We need to output a single action line. So next action: read package.json.[READ] package.json to inspect project setup..."
```

**Symptoms**:
- `[!] No action brackets found` error message
- Task created as `GENERAL` type instead of proper action type
- Malformed task descriptions

**Example** (line 597):
```
We need to read package.json.[READ] package.jsonWe need to actually execute read.[READ] package.json<|message|>...[READ] ./package.json<|message|>...[READ] ./app.js<|message|>...[READ] ./app.js
```

### Root Cause

1. **gpt-oss model adding `<|message|>` tokens** - This is a model-specific behavior where special tokens leak into the response
2. **Instruction following issues** - Model not following the strict format requirement
3. **Output repetition** - Model repeating the same action multiple times in one response

### Impact

- Tasks created with wrong action_type (GENERAL instead of READ/ADD/EDIT)
- Tasks have malformed descriptions containing multiple redundant actions
- Downstream agents confused by invalid task format

---

## Issue 3: Tool Retrieval System Overriding Explicit Constraints

### The Problem

**Location**: `rev/agents/context_provider.py:160-176`

The semantic retrieval system (`ContextBuilder.build()`) returns tools based on query similarity, **NOT** respecting the strict `candidate_tool_names` constraint.

**Current behavior**:
```python
bundle = builder.build(
    query=query,  # "add: create tests/user.test.js..."
    tool_universe=tool_universe,  # ALL tools
    tool_candidates=candidate_tool_names,  # ['write_file']
    top_k_tools=max_tools,  # 7
)
selected_tool_schemas = [t.schema for t in bundle.selected_tool_schemas]
# ❌ selected_tool_schemas may contain tools NOT in candidate_tool_names
```

**What happens**:
1. Query: "create tests/user.test.js with Jest/Supertest tests..."
2. Semantic search finds: `read_file` (score: 0.85), `run_all_analysis` (score: 0.82), `write_file` (score: 0.78)
3. Returns top 7 tools including `read_file` and `run_all_analysis`
4. **Ignores the fact that candidate_tool_names = ['write_file']**

**Fallback Logic** (lines 170-173):
```python
if not selected_tool_schemas:
    # Fall back to the explicit candidate list (preserve existing behavior).
    allowed = set(candidate_tool_names)
    selected_tool_schemas = [t for t in tool_universe if t.get("function", {}).get("name") in allowed][:max_tools]
```

This fallback **only activates if `selected_tool_schemas` is empty**, not if it contains wrong tools.

### Impact

- **Security risk**: Tools can be available that shouldn't be (e.g., destructive tools for read-only tasks)
- **Wrong tool selection**: LLM sees tools it shouldn't have access to
- **Inconsistent behavior**: Sometimes works (when retrieval returns correct tools), sometimes fails

---

## Solutions

### Solution 1: Strict Tool Filtering (CRITICAL)

**File**: `rev/agents/context_provider.py:169-176`

**Current**:
```python
selected_tool_schemas = [t.schema for t in bundle.selected_tool_schemas]
if not selected_tool_schemas:
    # Fall back to the explicit candidate list
    allowed = set(candidate_tool_names)
    selected_tool_schemas = [t for t in tool_universe if t.get("function", {}).get("name") in allowed][:max_tools]
```

**Fixed**:
```python
selected_tool_schemas = [t.schema for t in bundle.selected_tool_schemas]

# CRITICAL FIX: Always filter to only allowed candidate tools
# The retrieval system may return tools not in candidate_tool_names
# We must enforce the constraint regardless of what retrieval returns
if candidate_tool_names:
    allowed = set(candidate_tool_names)
    selected_tool_schemas = [
        t for t in selected_tool_schemas
        if t.get("function", {}).get("name") in allowed
    ]

# If filtering removed all tools, fall back to explicit candidate list
if not selected_tool_schemas and candidate_tool_names:
    allowed = set(candidate_tool_names)
    selected_tool_schemas = [
        t for t in tool_universe
        if t.get("function", {}).get("name") in allowed
    ][:max_tools]
```

**Impact**: Guarantees that ADD tasks only see `write_file`, EDIT tasks only see edit tools, etc.

### Solution 2: Enhanced System Prompt for Planner

**File**: `rev/execution/orchestrator.py` (planner prompt)

Add explicit formatting instructions:
```
CRITICAL OUTPUT FORMAT RULES:
1. Your response MUST be a single line starting with an action bracket
2. Valid action brackets: [READ], [ADD], [EDIT], [TEST], [REFACTOR], [CREATE_DIRECTORY], [ANALYZE]
3. Format: [ACTION_TYPE] description of task
4. Do NOT include explanations before or after the action line
5. Do NOT include special tokens like <|message|>
6. Do NOT repeat the action multiple times
7. Example CORRECT response: [READ] app.js to understand current implementation
8. Example WRONG response: We need to read app.js. [READ] app.js [READ] app.js

RESPOND NOW WITH ONLY THE ACTION LINE:
```

### Solution 3: Planner Output Validation

Add validation that detects and auto-corrects common gpt-oss issues:

```python
def clean_planner_response(response: str) -> str:
    """Clean up malformed planner responses from certain models."""
    # Remove model-specific tokens
    response = re.sub(r'<\|message\|>', '', response)

    # If response contains multiple [ACTION] patterns, take the first one
    action_pattern = r'\[(READ|ADD|EDIT|TEST|REFACTOR|CREATE_DIRECTORY|ANALYZE)\][^\[]*'
    matches = re.findall(action_pattern, response)
    if matches:
        return matches[0]  # Return first valid action

    return response
```

---

## Expected Results After Fixes

### Before Fixes
- 28 failed tasks / 21 completed = **57% failure rate**
- ADD tasks fail with "Write action completed without write tool execution"
- 15 planner format errors
- Execution stuck in retry loops

### After Fixes
- Expected failure rate: **<10%** (only legitimate failures)
- ADD tasks succeed on first attempt with `write_file`
- Planner format errors reduced by **90%+** (auto-correction handles gpt-oss quirks)
- Faster execution: ~10-15 minutes instead of 15+

---

## Test Plan

1. **Test strict tool filtering**:
   ```python
   # Test that ADD task only gets write_file
   task = Task(description="create test.js", action_type="add")
   _, tools, _ = build_context_and_tools(task, context, tool_universe=all_tools, candidate_tool_names=['write_file'])
   assert all(t['function']['name'] == 'write_file' for t in tools)
   assert len([t for t in tools if t['function']['name'] == 'read_file']) == 0
   ```

2. **Test planner output cleaning**:
   ```python
   # Test that gpt-oss malformed output gets cleaned
   response = "We need to read.[READ] app.js<|message|>[READ] app.js"
   cleaned = clean_planner_response(response)
   assert cleaned == "[READ] app.js"
   ```

3. **Integration test with gpt-oss model**:
   - Run same test-app task with fixes
   - Verify ADD tasks use write_file
   - Verify completion rate >90%
   - Verify execution time <15 minutes

---

## Files to Modify

1. **`rev/agents/context_provider.py`** (lines 169-176)
   - Add strict filtering of selected tools to candidate_tool_names
   - Ensure fallback logic activates when needed

2. **`rev/execution/orchestrator.py`** (planner prompt)
   - Add explicit format rules
   - Add examples of correct/wrong formats
   - Add model-specific guidance for gpt-oss

3. **`rev/execution/orchestrator.py`** (response parsing)
   - Add `clean_planner_response()` function
   - Apply cleaning before parsing action brackets
   - Handle model-specific tokens

---

## Priority

**CRITICAL**: Issue 1 (wrong tool selection) blocks all ADD tasks and must be fixed immediately.

**HIGH**: Issue 2 (planner format) causes task creation failures but can be partially mitigated with cleaning.

**MEDIUM**: Issue 3 (model-specific tokens) is already covered by fixing Issue 1 and 2.

---

## Summary

The gpt-oss model exposes a critical bug in the tool selection system where semantic retrieval overrides explicit tool constraints. This causes:
- ADD tasks to receive `read_file` instead of `write_file`
- 57% task failure rate
- Execution stuck in retry loops

**Fix**: Enforce strict filtering of retrieved tools to match `candidate_tool_names` constraint.

**Expected improvement**: Failure rate drops from 57% to <10%, execution completes successfully.
