# Prompt Optimization Integration

## Overview

The prompt optimization feature has been fully integrated into the Rev orchestrator pipeline. It analyzes user requests before planning and suggests improvements for vague or unclear prompts.

**Status**: ‚úÖ Fully Integrated and Tested

---

## Architecture

### Execution Flow

The prompt optimization phase is inserted into the orchestrator pipeline:

```
Orchestrator Flow:
    ‚Üì
[Phase 1: Learning Agent] (optional)
    ‚Üì
[Phase 2: Research Agent] (optional)
    ‚Üì
[Phase 2b: Prompt Optimization] ‚Üê NEW PHASE
    ‚îú‚îÄ Check if optimization needed (should_optimize_prompt)
    ‚îú‚îÄ Get LLM recommendations (get_prompt_recommendations)
    ‚îú‚îÄ Present user with options OR auto-optimize (prompt_optimization_dialog)
    ‚îî‚îÄ Update context.user_request with optimized prompt
    ‚Üì
[Phase 3: Planning Agent] (uses optimized request)
    ‚Üì
[Phase 4: Review Agent]
    ‚Üì
[Phase 5: Execution Agent]
```

### Configuration

#### OrchestratorConfig

Two new boolean fields control behavior:

```python
@dataclass
class OrchestratorConfig:
    # Prompt optimization
    enable_prompt_optimization: bool = True      # Default: enabled
    auto_optimize_prompt: bool = False           # Default: interactive mode
```

#### CLI Flags

Three new command-line flags:

```bash
# Enable prompt optimization (default)
rev --optimize-prompt "your request"

# Disable prompt optimization
rev --no-optimize-prompt "your request"

# Auto-optimize without asking user
rev --auto-optimize "your request"
```

#### Environment Variables

Control optimization via environment variables:

```bash
# Enable/disable optimization
export REV_OPTIMIZE_PROMPT=true|false

# Auto-optimize mode
export REV_AUTO_OPTIMIZE=true|false
```

**Priority Order** (highest to lowest):
1. CLI flags (`--optimize-prompt`, `--no-optimize-prompt`, `--auto-optimize`)
2. Environment variables (`REV_OPTIMIZE_PROMPT`, `REV_AUTO_OPTIMIZE`)
3. Default settings (optimization enabled, interactive mode)

---

## Implementation Details

### File Changes

#### 1. rev/execution/orchestrator.py

**Imports Added** (line 32):
```python
from rev.execution.prompt_optimizer import optimize_prompt_if_needed
```

**OrchestratorConfig Updated** (lines 70-72):
```python
# Prompt optimization
enable_prompt_optimization: bool = True
auto_optimize_prompt: bool = False
```

**run_orchestrated() Function Signature** (lines 1514-1515):
```python
enable_prompt_optimization: bool = True,
auto_optimize_prompt: bool = False,
```

**Orchestrator Phase 2b: Prompt Optimization** (lines 1044-1059):
```python
# Phase 2b: Prompt Optimization (optional)
if self.config.enable_prompt_optimization:
    original_request = self.context.user_request
    optimized_request, was_optimized = optimize_prompt_if_needed(
        self.context.user_request,
        auto_optimize=self.config.auto_optimize_prompt
    )
    if was_optimized:
        print(f"\n‚úì Request optimized for clarity")
        self.context.user_request = optimized_request
        self.context.add_insight("optimization", "prompt_optimized", True)
        self.context.agent_insights["prompt_optimization"] = {
            "optimized": True,
            "original": original_request[:100],
            "improved": optimized_request[:100]
        }
```

#### 2. rev/main.py

**Import Added** (line 6):
```python
import os
```

**CLI Flags Added** (lines 155-168):
```python
parser.add_argument(
    "--optimize-prompt",
    action="store_true",
    help="Enable prompt optimization - analyzes and suggests improvements to vague requests"
)
parser.add_argument(
    "--no-optimize-prompt",
    action="store_true",
    help="Disable prompt optimization"
)
parser.add_argument(
    "--auto-optimize",
    action="store_true",
    help="Auto-optimize prompts without asking user (implies --optimize-prompt)"
)
```

**CLI Flag Processing** (lines 202-220):
```python
# Determine prompt optimization settings
# Priority: CLI flags > Environment variables > defaults
enable_prompt_optimization = True  # Default
auto_optimize_prompt = False  # Default

# Check environment variables
if os.getenv("REV_OPTIMIZE_PROMPT", "").lower() == "false":
    enable_prompt_optimization = False
if os.getenv("REV_AUTO_OPTIMIZE", "").lower() == "true":
    auto_optimize_prompt = True

# Override with CLI flags (highest priority)
if args.no_optimize_prompt:
    enable_prompt_optimization = False
if args.optimize_prompt:
    enable_prompt_optimization = True
if args.auto_optimize:
    enable_prompt_optimization = True
    auto_optimize_prompt = True
```

**Debug Logging Updated** (lines 244-245):
```python
"prompt_optimization_enabled": enable_prompt_optimization,
"auto_optimize_prompt": auto_optimize_prompt,
```

**run_orchestrated() Call Updated** (lines 402-403):
```python
enable_prompt_optimization=enable_prompt_optimization,
auto_optimize_prompt=auto_optimize_prompt
```

---

## Usage Examples

### Example 1: Interactive Optimization (Default)

```bash
rev "Fix the bug"
```

Output:
```
=======================================================================
PROMPT OPTIMIZATION
=======================================================================

üìã Analyzing request for potential improvements...

üìä Analysis Results:
   Clarity Score: 4/10

   ‚ö†Ô∏è Potential Issues:
      - "Fix" is too vague - which problem?
      - No indication of scope

   ‚ùì Missing Information:
      - What's broken specifically?
      - What error messages?

   üí° Recommendations:
      - Describe the specific problem
      - Include error messages if available
      - Define expected behavior

üìù Suggested Improvement:
   Fix the /api/login endpoint that returns "Invalid credentials" error
   even with correct username/password.

=======================================================================
Options:
  [1] Use the suggested improvement
  [2] Keep the original request
  [3] Enter a custom request
=======================================================================

Choice [1-3]: 1

‚úì Using improved request.

‚úì Request optimized for clarity
```

### Example 2: Auto-Optimize (Non-Interactive)

```bash
rev --auto-optimize "Improve performance"
```

Result: System automatically uses the suggested improved prompt without asking.

### Example 3: Disable Optimization

```bash
rev --no-optimize-prompt "Fix the bug"
```

Result: Optimization phase is skipped; request is used as-is for planning.

### Example 4: Via Environment Variables

```bash
export REV_AUTO_OPTIMIZE=true
rev "Make it work"
```

Result: Automatically optimizes all vague requests.

---

## Test Coverage

**Test File**: `tests/test_prompt_optimization_integration.py`

**Test Classes** (21 tests, all passing):

1. **TestPromptOptimizationConfig** (4 tests)
   - Configuration defaults
   - Enabling/disabling optimization
   - Auto-optimize setting

2. **TestPromptOptimizationPhaseIntegration** (2 tests)
   - Optimization called during orchestration
   - Skipped when disabled

3. **TestPromptOptimizationPhaseExecution** (4 tests)
   - Vague requests detected
   - Clear requests not modified
   - Short requests flagged
   - LLM integration

4. **TestPromptOptimizationContextUpdate** (1 test)
   - Context properly updated with optimized request

5. **TestCLIIntegration** (3 tests)
   - All CLI flags parse correctly

6. **TestEnvironmentVariables** (3 tests)
   - Environment variables recognized
   - Priority logic correct

7. **TestPromptOptimizationWorkflow** (3 tests)
   - Complete workflows for different request types

8. **TestPromptOptimizationOutputFormat** (1 test)
   - Output format matches specification

**Running Tests**:
```bash
pytest tests/test_prompt_optimization_integration.py -v
```

All 21 tests pass successfully.

---

## Decision Logic

### When Is Optimization Triggered?

The `should_optimize_prompt()` function returns `True` if:

1. **Request is very short** (< 10 words)
   - Examples: "Fix auth", "Add feature", "Improve performance"

2. **Contains vague keywords** (and request < 30 words)
   - Keywords: "improve", "fix", "make", "do", "help", "try", "enhance", "optimize", "better", "good", "nice"
   - Example: "Improve the system"

3. **Multiple unrelated operations** (> 2 "and" separators)
   - Example: "Add auth and refactor utils and optimize DB"

### When Is Optimization Skipped?

The optimization phase is skipped if:

1. `enable_prompt_optimization=False` (via CLI or config)
2. Request scores as clear (score >= 7/10)
   - Clear requests include specific details, scope, and context

---

## Workflow Behavior

### Interactive Mode (Default)

```
User Request
    ‚Üì
should_optimize_prompt() ‚Üí True?
    ‚îú‚îÄ False ‚Üí Use original request
    ‚îî‚îÄ True ‚Üí Get LLM recommendations
        ‚Üì
    Display analysis to user:
    - Clarity score
    - Potential issues
    - Missing information
    - Recommendations
    - Suggested improvement
        ‚Üì
    User chooses:
    [1] Use suggestion
    [2] Keep original
    [3] Enter custom
        ‚Üì
    Final Request ‚Üí Planning Phase
```

### Auto-Optimize Mode

```
User Request
    ‚Üì
should_optimize_prompt() ‚Üí True?
    ‚îú‚îÄ False ‚Üí Use original request
    ‚îî‚îÄ True ‚Üí Get LLM recommendations
        ‚Üì
    Auto-use suggestion (no user interaction)
        ‚Üì
    Final Request ‚Üí Planning Phase
```

---

## Context and Insights

When optimization occurs, the orchestrator context is updated:

```python
self.context.agent_insights["prompt_optimization"] = {
    "optimized": True,
    "original": original_request[:100],      # Original request (first 100 chars)
    "improved": optimized_request[:100]      # Improved request (first 100 chars)
}
```

This allows downstream agents to understand what optimization occurred.

---

## Performance Considerations

### Token Usage

Prompt optimization adds one LLM call when:
- Optimization is enabled AND
- Request is detected as vague OR user explicitly enables it

**Estimated tokens per optimization**: ~300-500 tokens

**Impact**: Minimal for vague requests (which likely needed replanning anyway)

### Execution Flow

- **Without optimization**: Request ‚Üí Planning (immediately)
- **With optimization**: Request ‚Üí Analysis ‚Üí Recommendations ‚Üí Choice ‚Üí Planning

**Time overhead**: ~5-10 seconds for LLM analysis + user interaction

---

## Future Enhancements

### Potential Improvements

1. **Learning Integration**: Use historical optimization patterns to pre-optimize requests
2. **Confidence Scoring**: Surface confidence levels of improvements
3. **Multi-Language Support**: Optimize prompts in languages other than English
4. **Batch Optimization**: Optimize multiple requests in one LLM call
5. **Custom Rules**: Allow users to define vagueness criteria
6. **Automatic Mode Selection**: Choose interactive vs auto based on request clarity

### Configuration Extensions

```python
# Future additions to OrchestratorConfig
enable_prompt_optimization: bool = True
auto_optimize_prompt: bool = False
prompt_optimization_confidence_threshold: float = 0.7  # Only suggest if confident
prompt_optimization_show_reasoning: bool = False  # Show LLM reasoning
prompt_optimization_max_suggestions: int = 3  # Number of alternatives
```

---

## Troubleshooting

### Optimization Not Triggering

**Problem**: Optimization dialog never appears

**Solutions**:
1. Check if `enable_prompt_optimization=True` in config
2. Verify request is actually vague (< 10 words or contains vague keywords)
3. Check CLI flags (`--no-optimize-prompt` disables it)
4. Check environment variables (`REV_OPTIMIZE_PROMPT=false` disables it)

### LLM Error During Optimization

**Problem**: "Could not generate recommendations"

**Solutions**:
1. Verify Ollama is running and accessible
2. Check model supports tool calling (usually works with most models)
3. Check available tokens/rate limits
4. Use `--no-optimize-prompt` as workaround
5. Check debug logs: `rev --debug <request>`

### Auto-Optimize Not Working

**Problem**: `--auto-optimize` flag not applying

**Solutions**:
1. Verify flag is correctly spelled: `--auto-optimize` (not `--auto_optimize`)
2. Check for conflicting flags: `--no-optimize-prompt` takes precedence
3. Verify no environment variable override: `REV_AUTO_OPTIMIZE=false`

---

## Summary

The prompt optimization feature is now fully integrated into Rev's orchestrator pipeline:

‚úÖ **Core Feature**: Complete (7 tests)
‚úÖ **CLI Integration**: Complete (3 tests, 3 flags)
‚úÖ **Environment Variables**: Complete (2 env vars)
‚úÖ **Configuration**: Complete (2 config options)
‚úÖ **Test Coverage**: Complete (21 tests, 100% passing)
‚úÖ **Documentation**: Complete

**Files Modified**:
- `rev/execution/orchestrator.py` (import, config, phase insertion)
- `rev/main.py` (CLI flags, environment variables, parameter passing)

**Files Created**:
- `tests/test_prompt_optimization_integration.py` (21 comprehensive tests)
- `PROMPT_OPTIMIZATION_INTEGRATION.md` (this file)

**Usage**:
```bash
rev --optimize-prompt "vague request"     # Interactive
rev --auto-optimize "vague request"       # Non-interactive
rev --no-optimize-prompt "request"        # Disabled
```
