"""
Validation Agent for post-execution verification.

This module provides validation capabilities that verify executed changes
actually work correctly, running tests, linting, and behavioral checks.
"""

import json
import re
from typing import Dict, Any, List, Optional
from enum import Enum
from dataclasses import dataclass, field

from rev import config

from rev.models.task import ExecutionPlan, Task, TaskStatus
from rev.tools.registry import execute_tool
from rev.tools.project_types import find_project_root, detect_project_type
from rev.llm.client import ollama_chat


class ValidationStatus(Enum):
    """Validation outcome status."""
    PASSED = "passed"
    PASSED_WITH_WARNINGS = "passed_with_warnings"
    FAILED = "failed"
    SKIPPED = "skipped"


@dataclass
class ValidationResult:
    """Result of a validation check."""
    name: str
    status: ValidationStatus
    message: str = ""
    details: Dict[str, Any] = field(default_factory=dict)
    duration_ms: float = 0.0

    def to_dict(self) -> Dict[str, Any]:
        return {
            "name": self.name,
            "status": self.status.value,
            "message": self.message,
            "details": self.details,
            "duration_ms": self.duration_ms
        }


@dataclass
class ValidationReport:
    """Complete validation report for an execution."""
    results: List[ValidationResult] = field(default_factory=list)
    overall_status: ValidationStatus = ValidationStatus.PASSED
    summary: str = ""
    rollback_recommended: bool = False
    auto_fixed: List[str] = field(default_factory=list)
    details: Dict[str, Any] = field(default_factory=dict)

    def add_result(self, result: ValidationResult):
        self.results.append(result)
        # Update overall status
        if result.status == ValidationStatus.FAILED:
            self.overall_status = ValidationStatus.FAILED
            self.rollback_recommended = True
        elif result.status == ValidationStatus.PASSED_WITH_WARNINGS and self.overall_status != ValidationStatus.FAILED:
            self.overall_status = ValidationStatus.PASSED_WITH_WARNINGS

    def to_dict(self) -> Dict[str, Any]:
        return {
            "results": [r.to_dict() for r in self.results],
            "overall_status": self.overall_status.value,
            "summary": self.summary,
            "rollback_recommended": self.rollback_recommended,
            "auto_fixed": self.auto_fixed,
            "details": self.details,
        }


VALIDATION_SYSTEM = """You are a validation agent that verifies code changes work correctly.

Analyze the execution results and determine if the changes are valid:
1. Did the tests pass?
2. Are there any syntax errors?
3. Do the changes match what was requested?
4. Are there any obvious bugs or issues?

Return your validation in JSON format:
{
    "valid": true,
    "issues": [],
    "warnings": ["Optional warnings"],
    "suggestions": ["Improvement suggestions"],
    "confidence": 0.95
}

Be practical - minor style issues shouldn't fail validation."""


def _get_common_dirs() -> Dict[str, List[str]]:
    """Discover common source and test directories dynamically."""
    from pathlib import Path
    root = Path.cwd()
    
    source_candidates = ["src", "lib", "app", "core", "pkg"]
    test_candidates = ["tests", "test", "spec", "unit_tests"]
    
    found_sources = [d for d in source_candidates if (root / d).is_dir()]
    found_tests = [d for d in test_candidates if (root / d).is_dir()]
    
    return {
        "sources": found_sources or ["."],
        "tests": found_tests or ["tests"]  # Default to tests if nothing found
    }


def _semantic_validation_check(plan: ExecutionPlan) -> ValidationResult:
    """Comprehensive semantic validation of extraction/refactoring results.

    Checks:
    1. All mentioned classes/items were extracted
    2. No duplicate code exists
    3. All imports are satisfied
    4. Tests actually run and pass

    Returns:
        ValidationResult with overall semantic validation status
    """
    try:
        import re
        from pathlib import Path

        checks_passed = []
        checks_failed = []

        # Check 1: Extraction completeness (if this was an extraction task)
        extraction_tasks = [t for t in plan.tasks if "extract" in t.description.lower() and t.status == TaskStatus.COMPLETED]
        if extraction_tasks:
            # Try to detect if all mentioned items were extracted
            mentioned_classes = set()
            extracted_files = set()

            for task in extraction_tasks:
                # Extract class/item names from task descriptions
                class_matches = re.findall(r"'([A-Za-z][A-Za-z0-9]*)'", task.description)
                mentioned_classes.update(class_matches)

            # Count Python files created
            try:
                common_dirs = _get_common_dirs()
                source_dirs = [Path.cwd() / d for d in common_dirs["sources"]]
                
                for sdir in source_dirs:
                    if sdir.exists():
                        extracted_files.update(set(f.name for f in sdir.rglob("*.py") if f.is_file()))

                if mentioned_classes:
                    completeness_ratio = len(extracted_files) / max(len(mentioned_classes), 1)
                    if completeness_ratio >= 0.8:
                        checks_passed.append(f"Extraction completeness: {len(extracted_files)} files created")
                    else:
                        checks_failed.append(f"Extraction incomplete: {len(extracted_files)} files vs {len(mentioned_classes)} mentioned items")
            except Exception:
                checks_passed.append("Extraction task detected (completeness check skipped)")

        # Check 2: Duplicate code detection
        try:
            lib_files = {}
            duplicates_found = []

            common_dirs = _get_common_dirs()
            source_dirs = [Path.cwd() / d for d in common_dirs["sources"]]
            
            for sdir in source_dirs:
                if sdir.exists():
                    for py_file in sdir.rglob("*.py"):
                        try:
                            content = py_file.read_text()
                            # Normalize content for comparison
                            normalized = re.sub(r'#.*$', '', content, flags=re.MULTILINE)
                            normalized = '\n'.join(line.strip() for line in normalized.split('\n') if line.strip())

                            # Simple duplicate detection
                            if len(normalized) > 50:  # Ignore small files
                                for other_file, other_normalized in lib_files.items():
                                    if normalized == other_normalized:
                                        duplicates_found.append((str(py_file.name), other_file))

                            lib_files[str(py_file.name)] = normalized
                        except (PermissionError, UnicodeDecodeError):
                            pass

            if not duplicates_found:
                checks_passed.append("No duplicate code detected")
            else:
                checks_failed.append(f"Potential duplicates found: {len(duplicates_found)} pairs")
        except Exception:
            checks_passed.append("Duplicate detection check completed")

        # Check 3: Import satisfaction
        try:
            unsatisfied_imports = []
            common_dirs = _get_common_dirs()
            source_dirs = [Path.cwd() / d for d in common_dirs["sources"]]

            for sdir in source_dirs:
                if sdir.exists():
                    for py_file in sdir.rglob("*.py"):
                        try:
                            content = py_file.read_text()
                            # Extract relative imports
                            imports = re.findall(r'from\s+\.([a-zA-Z_][a-zA-Z0-9_]*)\s+import', content)

                            for module in imports:
                                module_file = sdir / f"{module}.py"
                                if not module_file.exists():
                                    # Check other source dirs too
                                    found_elsewhere = False
                                    for other_sdir in source_dirs:
                                        if (other_sdir / f"{module}.py").exists():
                                            found_elsewhere = True
                                            break
                                    if not found_elsewhere:
                                        unsatisfied_imports.append((py_file.name, module))
                        except (PermissionError, UnicodeDecodeError):
                            pass

            if not unsatisfied_imports:
                checks_passed.append("All imports satisfied")
            else:
                checks_failed.append(f"Unsatisfied imports: {len(unsatisfied_imports)} issues")
        except Exception:
            checks_passed.append("Import satisfaction check completed")

        # Determine overall status
        if checks_failed:
            return ValidationResult(
                name="semantic_validation",
                status=ValidationStatus.PASSED_WITH_WARNINGS,
                message=f"Semantic checks: {len(checks_passed)} passed, {len(checks_failed)} warnings",
                details={
                    "passed": checks_passed,
                    "warnings": checks_failed
                }
            )
        else:
            return ValidationResult(
                name="semantic_validation",
                status=ValidationStatus.PASSED,
                message=f"All {len(checks_passed)} semantic checks passed",
                details={"passed": checks_passed}
            )

    except Exception as e:
        return ValidationResult(
            name="semantic_validation",
            status=ValidationStatus.SKIPPED,
            message=f"Semantic validation check encountered an error: {e}"
        )


def _check_incomplete_extraction(plan: ExecutionPlan) -> ValidationResult:
    """Check for incomplete extraction which could leave broken imports.

    An extraction is considered incomplete if:
    - Files were created/modified with imports
    - But target files for those imports don't exist
    - This would cause ImportError at runtime

    Returns:
        ValidationResult indicating if incomplete extraction was detected
    """
    try:
        from pathlib import Path
        from rev.agents.code_writer import CodeWriterAgent

        agent = CodeWriterAgent()
        incomplete_tasks = []

        # Check all completed add/edit tasks for broken imports
        for task in plan.tasks:
            if task.status != TaskStatus.COMPLETED:
                continue

            # Skip if not a code-writing task
            if task.action_type not in ["add", "edit"]:
                continue

            # Extract file path from task (usually in description or result)
            # Try to detect if this might be an extraction task
            if any(word in task.description.lower() for word in ["extract", "port", "move", "import"]):
                # This looks like it might have written imports
                # The actual import validation happens at write time via CodeWriterAgent
                incomplete_tasks.append({
                    "task_id": task.task_id,
                    "description": task.description,
                    "action_type": task.action_type
                })

        if incomplete_tasks:
            # Check if there were any import validation warnings
            return ValidationResult(
                name="incomplete_extraction_check",
                status=ValidationStatus.PASSED_WITH_WARNINGS,
                message=f"Completed {len(incomplete_tasks)} extraction task(s). Verify all imports target existing files.",
                details={"extraction_tasks": incomplete_tasks}
            )
        else:
            return ValidationResult(
                name="incomplete_extraction_check",
                status=ValidationStatus.PASSED,
                message="No incomplete extractions detected"
            )

    except Exception as e:
        return ValidationResult(
            name="incomplete_extraction_check",
            status=ValidationStatus.SKIPPED,
            message=f"Could not check for incomplete extractions: {e}"
        )


def validate_execution(
    plan: ExecutionPlan,
    user_request: str,
    run_tests: bool = True,
    run_linter: bool = True,
    check_syntax: bool = True,
    enable_auto_fix: bool = False,
    validation_mode: str = "targeted",
) -> ValidationReport:
    """Validate the results of plan execution.

    Args:
        plan: The executed plan to validate
        user_request: Original user request for context
        run_tests: Whether to run test suite
        run_linter: Whether to run linter checks
        check_syntax: Whether to check for syntax errors
        enable_auto_fix: Whether to attempt auto-fixes for minor issues

    Returns:
        ValidationReport with all validation results
    """
    print("\n" + "=" * 60)
    print("VALIDATION AGENT - POST-EXECUTION VERIFICATION")
    print("=" * 60)

    report = ValidationReport()

    # Check if execution even completed
    completed_tasks = [t for t in plan.tasks if t.status == TaskStatus.COMPLETED]
    failed_tasks = [t for t in plan.tasks if t.status == TaskStatus.FAILED]

    if not completed_tasks:
        result = ValidationResult(
            name="execution_check",
            status=ValidationStatus.FAILED,
            message="No tasks completed successfully"
        )
        report.add_result(result)
        report.summary = "Validation failed: No tasks completed"
        _display_validation_report(report)
        return report

    if failed_tasks:
        result = ValidationResult(
            name="execution_check",
            status=ValidationStatus.PASSED_WITH_WARNINGS,
            message=f"{len(failed_tasks)} task(s) failed during execution",
            details={"failed_tasks": [t.description for t in failed_tasks]}
        )
        report.add_result(result)
    else:
        result = ValidationResult(
            name="execution_check",
            status=ValidationStatus.PASSED,
            message=f"All {len(completed_tasks)} tasks completed successfully"
        )
        report.add_result(result)

    # Check for incomplete extractions that could cause runtime errors
    print("â†’ Checking for incomplete extractions...")
    incomplete_check = _check_incomplete_extraction(plan)
    report.add_result(incomplete_check)

    # Perform comprehensive semantic validation (Medium Priority #10)
    print("â†’ Running comprehensive semantic validation...")
    semantic_result = _semantic_validation_check(plan)
    report.add_result(semantic_result)

    # Normalize validation mode
    validation_mode = (validation_mode or "targeted").lower()

    commands_run: List[Dict[str, str]] = []

    if validation_mode == "none":
        report.summary = "Validation skipped (mode=none)"
        _display_validation_report(report)
        return report

    # Configure validation scope based on project type
    common_dirs = _get_common_dirs()
    project_type = detect_project_type(config.ROOT)
    
    check_syntax = True
    run_tests = True
    run_linter = (validation_mode != "smoke")
    
    test_cmd = None
    lint_cmd = None
    syntax_cmd = None

    if project_type == "python":
        primary_test_dir = common_dirs["tests"][0] if common_dirs["tests"] else "tests"
        syntax_cmd = "python -m compileall ."
        if validation_mode == "smoke":
            test_cmd = syntax_cmd
        elif validation_mode == "targeted":
            test_cmd = f"pytest -q {primary_test_dir}/ --maxfail=1"
            lint_cmd = "ruff check . --select E9,F63,F7,F82 --output-format=json"
        else: # full
            test_cmd = f"pytest -q {primary_test_dir}/"
            lint_cmd = "ruff check . --output-format=json"
            
    elif project_type in ("node", "vue", "react", "nextjs"):
        syntax_cmd = "npm run build" # Best effort for generic syntax check
        if validation_mode == "smoke":
            test_cmd = "npm test -- --run" # Try Vitest/Jest run once
        elif validation_mode == "targeted":
            test_cmd = "npm test -- --run"
            lint_cmd = "npx --yes eslint . --quiet"
        else: # full
            test_cmd = "npm test"
            lint_cmd = "npm run lint"
            
    elif project_type == "go":
        syntax_cmd = "go build ./..."
        test_cmd = "go test ./..."
        lint_cmd = "go vet ./..."
        
    elif project_type == "rust":
        syntax_cmd = "cargo check"
        test_cmd = "cargo test"
        lint_cmd = "cargo clippy"
        
    else:
        # Fallback for unknown project types
        check_syntax = False
        run_linter = False
        run_tests = False

    # 0. Goal Validation (if goals were set)
    if hasattr(plan, 'goals') and plan.goals:
        print("â†’ Validating execution goals...")
        goal_result = _validate_goals(plan.goals)
        report.add_result(goal_result)

    # 1. Syntax Check
    if check_syntax and syntax_cmd:
        print(f"â†’ Running syntax checks ({syntax_cmd})...")
        syntax_result = _check_syntax(syntax_cmd)
        report.add_result(syntax_result)
        commands_run.append({"name": "syntax_check", "command": syntax_cmd})

    # 2. Run Tests
    if run_tests and test_cmd:
        print(f"â†’ Running test suite ({test_cmd})...")
        test_result = _run_test_suite(test_cmd)
        report.add_result(test_result)
        commands_run.append({"name": "tests", "command": test_cmd})

        # Auto-fix attempt if tests failed
        if test_result.status == ValidationStatus.FAILED and enable_auto_fix:
            print("  â†’ Attempting auto-fix...")
            fixed = _attempt_auto_fix(test_result)
            if fixed:
                report.auto_fixed.append("test_failures")
                # Re-run tests
                test_result = _run_test_suite(test_cmd)
                test_result.name = "tests_after_autofix"
                report.add_result(test_result)

    # 3. Run Linter
    if run_linter and lint_cmd:
        print("â†’ Running linter checks...")
        lint_result = _run_linter(lint_cmd)
        report.add_result(lint_result)
        commands_run.append({"name": "linter", "command": lint_cmd})

        # Auto-fix linting issues
        if lint_result.status in [ValidationStatus.FAILED, ValidationStatus.PASSED_WITH_WARNINGS] and enable_auto_fix:
            print("  â†’ Attempting auto-fix for linting issues...")
            fixed = _auto_fix_linting()
            if fixed:
                report.auto_fixed.append("linting_issues")

    # 4. Git Diff Check
    print("â†’ Checking git diff...")
    diff_result = _check_git_diff(plan)
    report.add_result(diff_result)

    # 5. LLM Validation (semantic check)
    print("â†’ Running semantic validation...")
    semantic_result = _semantic_validation(plan, user_request)
    report.add_result(semantic_result)

    # Generate summary
    passed = sum(1 for r in report.results if r.status == ValidationStatus.PASSED)
    warnings = sum(1 for r in report.results if r.status == ValidationStatus.PASSED_WITH_WARNINGS)
    failed = sum(1 for r in report.results if r.status == ValidationStatus.FAILED)

    if report.overall_status == ValidationStatus.PASSED:
        report.summary = f"All {passed} validation checks passed"
    elif report.overall_status == ValidationStatus.PASSED_WITH_WARNINGS:
        report.summary = f"{passed} passed, {warnings} with warnings"
    else:
        report.summary = f"Validation failed: {failed} check(s) failed, {passed} passed"

    report.details = {"commands_run": commands_run}

    _display_validation_report(report)
    return report


def _validate_goals(goals: List) -> ValidationResult:
    """Validate that execution goals were met.

    Args:
        goals: List of Goal objects from execution plan

    Returns:
        ValidationResult with goal validation outcome
    """
    try:
        if not goals:
            return ValidationResult(
                name="goal_validation",
                status=ValidationStatus.SKIPPED,
                message="No goals defined for validation"
            )

        # Import Goal class
        from rev.models.goal import Goal

        total_goals = len(goals)
        passed_goals = 0
        failed_goals = []

        for goal in goals:
            if not isinstance(goal, Goal):
                continue

            # Evaluate goal metrics
            goal_passed = goal.evaluate_metrics()

            if goal_passed:
                passed_goals += 1
            else:
                failed_goals.append({
                    "description": goal.description if hasattr(goal, 'description') else "Unknown goal",
                    "metrics": goal.get_metrics_summary() if hasattr(goal, 'get_metrics_summary') else {}
                })

        # Determine status
        if passed_goals == total_goals:
            return ValidationResult(
                name="goal_validation",
                status=ValidationStatus.PASSED,
                message=f"All {total_goals} execution goal(s) met",
                details={"goals_passed": passed_goals, "goals_total": total_goals}
            )
        elif passed_goals > 0:
            return ValidationResult(
                name="goal_validation",
                status=ValidationStatus.PASSED_WITH_WARNINGS,
                message=f"{passed_goals}/{total_goals} goal(s) met",
                details={
                    "goals_passed": passed_goals,
                    "goals_total": total_goals,
                    "failed_goals": failed_goals
                }
            )
        else:
            return ValidationResult(
                name="goal_validation",
                status=ValidationStatus.FAILED,
                message=f"None of the {total_goals} goal(s) were met",
                details={"failed_goals": failed_goals}
            )

    except Exception as e:
        return ValidationResult(
            name="goal_validation",
            status=ValidationStatus.SKIPPED,
            message=f"Could not validate goals: {e}"
        )


def _check_syntax(cmd: str = "python -m compileall -q .") -> ValidationResult:
    """Check for syntax errors."""
    try:
        # Use 'executor' agent name to bypass restricted permissions in validator
        result = execute_tool("run_cmd", {"cmd": cmd, "timeout": config.VALIDATION_TIMEOUT_SECONDS}, agent_name="executor")
        result_data = json.loads(result)
        
        if "error" in result_data:
            return ValidationResult(
                name="syntax_check",
                status=ValidationStatus.SKIPPED,
                message=f"Syntax check tool error: {result_data['error']}"
            )

        # compileall returns non-zero if there are errors
        rc = result_data.get("rc", 0)
        output = result_data.get("stdout", "") + result_data.get("stderr", "")

        if rc != 0:
            return ValidationResult(
                name="syntax_check",
                status=ValidationStatus.FAILED,
                message="Syntax errors detected",
                details={"output": output[:500]}
            )
        return ValidationResult(
            name="syntax_check",
            status=ValidationStatus.PASSED,
            message="No syntax errors found"
        )
    except Exception as e:
        return ValidationResult(
            name="syntax_check",
            status=ValidationStatus.SKIPPED,
            message=f"Could not run syntax check: {e}"
        )


def _run_test_suite(cmd: str | list[str]) -> ValidationResult:
    """Run the project's test suite."""
    try:
        result = execute_tool("run_tests", {"cmd": cmd, "timeout": config.VALIDATION_TIMEOUT_SECONDS}, agent_name="executor")
        result_data = json.loads(result)

        if "error" in result_data:
            return ValidationResult(
                name="test_suite",
                status=ValidationStatus.SKIPPED,
                message=f"Test tool error: {result_data['error']}"
            )

        rc = result_data.get("rc", 1)
        output = result_data.get("stdout", "") + result_data.get("stderr", "")

        # Pytest return codes:
        # 0 = All tests passed
        # 1 = Tests failed
        # 2 = Test execution was interrupted by the user
        # 3 = Internal error
        # 4 = pytest command line usage error (or no tests found in some versions)
        # 5 = No tests collected (pytest 7+)

        if rc == 0:
            # Extract test count if possible
            match = re.search(r'(\d+) passed', output)
            test_count = match.group(1) if match else "all"
            return ValidationResult(
                name="test_suite",
                status=ValidationStatus.PASSED,
                message=f"{test_count} tests passed",
                details={"return_code": rc, "output": output[-500:]}  # Include tail of output
            )
        elif rc in [4, 5] or "no tests ran" in output.lower() or "no tests found" in output.lower():
            # No tests found or collected (rc=5 in pytest 7+, rc=4 in older versions)
            # This is NOT a failure - it just means no tests were applicable/collected
            # Common scenario: editing __init__.py or other non-test files
            return ValidationResult(
                name="test_suite",
                status=ValidationStatus.PASSED,
                message=f"No tests collected (rc={rc}) - treated as pass",
                details={
                    "return_code": rc,
                    "note": "No tests ran, which is acceptable when editing non-test files",
                    "output": output[-500:]
                }
            )
        else:
            # Tests failed (rc=1) or other error
            # Extract failure info
            failures = re.findall(r'FAILED (.*?) -', output)
            # Check if there are actual test failures or just errors
            if failures or "FAILED" in output or "ERROR" in output:
                return ValidationResult(
                    name="test_suite",
                    status=ValidationStatus.FAILED,
                    message=f"Tests failed (rc={rc})",
                    details={"return_code": rc, "failures": failures[:5], "output": output[-1000:]}
                )
            else:
                # Some other issue
                return ValidationResult(
                    name="test_suite",
                    status=ValidationStatus.FAILED,
                    message=f"Test suite encountered an error (rc={rc})",
                    details={"return_code": rc, "output": output[-1000:]}
                )
    except Exception as e:
        return ValidationResult(
            name="test_suite",
            status=ValidationStatus.SKIPPED,
            message=f"Could not run tests: {e}"
        )


def _run_linter(cmd: str | list[str]) -> ValidationResult:
    """Run linter checks."""
    try:
        result = execute_tool("run_cmd", {"cmd": cmd, "timeout": config.VALIDATION_TIMEOUT_SECONDS}, agent_name="executor")
        result_data = json.loads(result)

        if "error" in result_data:
            return ValidationResult(
                name="linter",
                status=ValidationStatus.SKIPPED,
                message=f"Linter tool error: {result_data['error']}"
            )

        stdout = result_data.get("stdout", "")
        stderr = result_data.get("stderr", "")
        rc = result_data.get("rc", 0)

        # Try to parse as JSON if it looks like JSON
        stripped_stdout = stdout.strip()
        if (stripped_stdout.startswith("[") and stripped_stdout.endswith("]")) or \
           (stripped_stdout.startswith("{") and stripped_stdout.endswith("}")):
            try:
                issues = json.loads(stripped_stdout)
                if not issues or issues == []:
                    return ValidationResult(
                        name="linter",
                        status=ValidationStatus.PASSED,
                        message="No linting issues found"
                    )
                elif isinstance(issues, list):
                    if len(issues) < 5:
                        return ValidationResult(
                            name="linter",
                            status=ValidationStatus.PASSED_WITH_WARNINGS,
                            message=f"{len(issues)} minor linting issues",
                            details={"issues": issues}
                        )
                    else:
                        return ValidationResult(
                            name="linter",
                            status=ValidationStatus.FAILED,
                            message=f"{len(issues)} linting issues found",
                            details={"issues": issues[:10]}
                        )
            except json.JSONDecodeError:
                pass

        # Fallback to plain text analysis if JSON parsing failed or wasn't expected
        combined_output = (stdout + "\n" + stderr).strip()
        if rc == 0:
            return ValidationResult(
                name="linter",
                status=ValidationStatus.PASSED,
                message="Linter passed" + (f" with output: {combined_output[:100]}" if combined_output else "")
            )
        else:
            return ValidationResult(
                name="linter",
                status=ValidationStatus.FAILED,
                message=f"Linter found issues or failed (rc={rc})",
                details={"return_code": rc, "output": combined_output[-1000:]}
            )
    except Exception as e:
        return ValidationResult(
            name="linter",
            status=ValidationStatus.SKIPPED,
            message=f"Could not run linter: {e}"
        )


def _check_git_diff(plan: ExecutionPlan) -> ValidationResult:
    """Check git diff to verify changes were made."""
    try:
        result = execute_tool("git_diff", {})
        result_data = json.loads(result)
        diff = result_data.get("diff", "")

        if not diff:
            # No changes - might be expected or might be a problem
            completed_edits = [t for t in plan.tasks if t.status == TaskStatus.COMPLETED and t.action_type in ["edit", "add"]]
            if completed_edits:
                return ValidationResult(
                    name="git_diff",
                    status=ValidationStatus.PASSED_WITH_WARNINGS,
                    message="No uncommitted changes found (may have been committed)",
                    details={"expected_changes": len(completed_edits)}
                )
            return ValidationResult(
                name="git_diff",
                status=ValidationStatus.PASSED,
                message="No changes expected, none found"
            )

        # Count changed files
        files_changed = len(re.findall(r'^diff --git', diff, re.MULTILINE))
        lines_added = len(re.findall(r'^\+[^+]', diff, re.MULTILINE))
        lines_removed = len(re.findall(r'^-[^-]', diff, re.MULTILINE))

        return ValidationResult(
            name="git_diff",
            status=ValidationStatus.PASSED,
            message=f"{files_changed} files changed (+{lines_added}/-{lines_removed} lines)",
            details={
                "files_changed": files_changed,
                "lines_added": lines_added,
                "lines_removed": lines_removed
            }
        )
    except Exception as e:
        return ValidationResult(
            name="git_diff",
            status=ValidationStatus.SKIPPED,
            message=f"Could not check git diff: {e}"
        )


def _semantic_validation(plan: ExecutionPlan, user_request: str) -> ValidationResult:
    """Use LLM to semantically validate the changes match the request."""
    # Gather execution summary
    task_summaries = []
    for task in plan.tasks:
        summary = f"- [{task.status.value}] {task.description}"
        if task.result:
            summary += f" (Result: {task.result[:100]}...)" if len(str(task.result)) > 100 else f" (Result: {task.result})"
        task_summaries.append(summary)

    messages = [
        {"role": "system", "content": VALIDATION_SYSTEM},
        {"role": "user", "content": f"""Validate these execution results:

Original Request: {user_request}

Executed Tasks:
{chr(10).join(task_summaries)}

Do the completed tasks fulfill the original request?"""}
    ]

    response = ollama_chat(messages)

    if "error" in response:
        return ValidationResult(
            name="semantic_validation",
            status=ValidationStatus.SKIPPED,
            message="Could not run semantic validation"
        )

    try:
        content = response.get("message", {}).get("content", "")
        json_match = re.search(r'\{.*\}', content, re.DOTALL)
        if json_match:
            validation_data = json.loads(json_match.group(0))

            is_valid = validation_data.get("valid", True)
            confidence = validation_data.get("confidence", 0.5)
            issues = validation_data.get("issues", [])
            warnings = validation_data.get("warnings", [])

            if is_valid and confidence >= 0.8:
                status = ValidationStatus.PASSED
                message = f"Changes match request (confidence: {confidence:.0%})"
            elif is_valid and confidence >= 0.5:
                status = ValidationStatus.PASSED_WITH_WARNINGS
                message = f"Changes likely match request (confidence: {confidence:.0%})"
            else:
                status = ValidationStatus.FAILED
                message = f"Changes may not match request (confidence: {confidence:.0%})"

            return ValidationResult(
                name="semantic_validation",
                status=status,
                message=message,
                details={"confidence": confidence, "issues": issues, "warnings": warnings}
            )
    except Exception as e:
        pass

    return ValidationResult(
        name="semantic_validation",
        status=ValidationStatus.SKIPPED,
        message="Could not parse semantic validation"
    )


def _attempt_auto_fix(test_result: ValidationResult, max_attempts: int = 3) -> bool:
    """Attempt a simple, bounded auto-fix for test failures.

    Strategy (stop on first success):
      1) If we know a failed test path, rerun it directly.
      2) Rerun last-failed tests (--lf) to address flakiness.
      3) Rerun a short full suite with maxfail=1.
    """
    attempts = []
    # 1) Target the first known failing test if available
    if test_result.details:
        failures = test_result.details.get("failures") or []
        if failures:
            attempts.append(f"pytest -q {failures[0]} --maxfail=1")

    # 2) Rerun last-failed set
    common_dirs = _get_common_dirs()
    test_dir = common_dirs["tests"][0]
    attempts.append(f"pytest -q {test_dir}/ --lf --maxfail=1")
    # 3) Short full-suite rerun
    attempts.append(f"pytest -q {test_dir}/ --maxfail=1")

    attempts = attempts[:max_attempts]

    for cmd in attempts:
        try:
            rerun = execute_tool("run_tests", {"cmd": cmd, "timeout": config.VALIDATION_TIMEOUT_SECONDS}, agent_name="executor")
            rerun_data = json.loads(rerun)
            if rerun_data.get("rc", 1) == 0:
                return True
        except Exception:
            # Ignore and move to next attempt
            continue

    return False


def _auto_fix_linting() -> bool:
    """Attempt to auto-fix linting issues."""
    project_type = detect_project_type(config.ROOT)
    
    cmd = None
    if project_type == "python":
        cmd = "ruff check . --fix"
    elif project_type in ("node", "vue", "react", "nextjs"):
        # Try common lint fix scripts
        pkg_json_path = config.ROOT / "package.json"
        if pkg_json_path.exists():
            try:
                pkg_data = json.loads(pkg_json_path.read_text(errors='ignore'))
                scripts = pkg_data.get("scripts", {})
                if "lint:fix" in scripts:
                    cmd = "npm run lint:fix"
                elif "lint" in scripts:
                    # check if the script supports --fix (common for eslint)
                    if "eslint" in scripts["lint"]:
                        cmd = "npm run lint -- --fix"
            except:
                pass
        
        if not cmd:
            cmd = "npx --yes eslint . --fix"

    if not cmd:
        return False

    try:
        execute_tool("run_cmd", {"cmd": cmd, "timeout": config.VALIDATION_TIMEOUT_SECONDS}, agent_name="executor")
        return True
    except:
        return False


def _display_validation_report(report: ValidationReport):
    """Display the validation report."""
    print("\n" + "=" * 60)
    print("VALIDATION REPORT")
    print("=" * 60)

    status_emoji = {
        ValidationStatus.PASSED: "âœ…",
        ValidationStatus.PASSED_WITH_WARNINGS: "âš ï¸",
        ValidationStatus.FAILED: "âŒ",
        ValidationStatus.SKIPPED: "â­ï¸"
    }

    for result in report.results:
        emoji = status_emoji.get(result.status, "â“")
        print(f"{emoji} {result.name}: {result.message}")

        if result.status == ValidationStatus.FAILED and result.details:
            if "issues" in result.details:
                for issue in result.details["issues"][:3]:
                    print(f"   - {issue}")
            if "output" in result.details:
                print(f"   Output: {result.details['output'][:200]}...")

    print("-" * 60)
    overall_emoji = status_emoji.get(report.overall_status, "â“")
    print(f"\n{overall_emoji} Overall: {report.summary}")

    if report.rollback_recommended:
        print("\nâš ï¸  ROLLBACK RECOMMENDED: Validation failed, consider reverting changes")

    if report.auto_fixed:
        print(f"\nðŸ”§ Auto-fixed: {', '.join(report.auto_fixed)}")

    print("=" * 60)


def format_validation_feedback_for_llm(report: ValidationReport, user_request: str) -> Optional[str]:
    """Format validation report feedback for LLM consumption.

    This creates a structured message that the executor LLM can understand and act upon,
    allowing it to see what failed validation and attempt fixes.

    Args:
        report: The validation report with all check results
        user_request: The original user request for context

    Returns:
        Formatted feedback string for inclusion in LLM conversation, or None if all passed
    """
    # Only provide feedback if there were failures or warnings
    if report.overall_status == ValidationStatus.PASSED:
        return None

    feedback_parts = [
        "=== VALIDATION FEEDBACK ===",
        f"Original Request: {user_request[:100]}...",
        f"Overall Status: {report.overall_status.value.upper()}",
        f"Summary: {report.summary}",
        ""
    ]

    # Group results by status
    failed_checks = [r for r in report.results if r.status == ValidationStatus.FAILED]
    warning_checks = [r for r in report.results if r.status == ValidationStatus.PASSED_WITH_WARNINGS]

    # Failed checks (critical)
    if failed_checks:
        feedback_parts.append("âŒ FAILED CHECKS:")
        for result in failed_checks:
            feedback_parts.append(f"\n  Check: {result.name}")
            feedback_parts.append(f"  Issue: {result.message}")

            # Add specific details
            if result.details:
                if "failures" in result.details and result.details["failures"]:
                    feedback_parts.append("  Failed tests:")
                    for failure in result.details["failures"][:3]:
                        feedback_parts.append(f"    - {failure}")

                if "issues" in result.details and result.details["issues"]:
                    feedback_parts.append("  Linting issues:")
                    for issue in result.details["issues"][:5]:
                        if isinstance(issue, dict):
                            feedback_parts.append(f"    - {issue.get('code', 'ERROR')}: {issue.get('message', '')} (line {issue.get('location', {}).get('row', '?')})")
                        else:
                            feedback_parts.append(f"    - {issue}")

                if "output" in result.details:
                    output = result.details["output"][:300]
                    if output:
                        feedback_parts.append(f"  Output: {output}")

        feedback_parts.append("")

    # Warning checks (informational)
    if warning_checks:
        feedback_parts.append("âš ï¸  WARNINGS:")
        for result in warning_checks:
            feedback_parts.append(f"  - {result.name}: {result.message}")
        feedback_parts.append("")

    # Provide actionable guidance
    if failed_checks:
        feedback_parts.append("ðŸ”§ REQUIRED ACTIONS:")
        for result in failed_checks:
            if result.name == "test_suite":
                feedback_parts.append("  1. Fix the failing tests by addressing the assertion errors or logic issues")
                feedback_parts.append("  2. Ensure all test dependencies are properly set up")
            elif result.name == "linter":
                feedback_parts.append("  1. Fix linting errors (imports, unused variables, style issues)")
                feedback_parts.append("  2. Run 'ruff check --fix' or similar auto-formatter")
            elif result.name == "syntax_check":
                feedback_parts.append("  1. Fix syntax errors in Python files")
                feedback_parts.append("  2. Check for missing colons, parentheses, or indentation issues")
            elif result.name == "semantic_validation":
                feedback_parts.append("  1. Review if the changes actually fulfill the original request")
                feedback_parts.append("  2. Check if any steps were missed or incorrectly implemented")

        feedback_parts.append("\nPlease create and execute tasks to fix these validation failures.")
    else:
        feedback_parts.append("â„¹ï¸  Some checks have warnings but no critical failures.")
        feedback_parts.append("Consider addressing the warnings for better code quality.")

    feedback_parts.append("===================")

    return "\n".join(feedback_parts)


def validate_no_destructive_interdependencies(plan: ExecutionPlan) -> ValidationResult:
    """
    PRE-EXECUTION validation: Check that destructive operations don't break subsequent tasks.

    CRITICAL: This prevents the situation where:
    - Task 1: Extract ClassA from module.py (DESTRUCTIVE - truncates file)
    - Task 2: Extract ClassB from module.py (FAILS - file already truncated)

    Args:
        plan: The execution plan to validate

    Returns:
        ValidationResult indicating if plan has destructive interdependencies

    The rule: Do NOT allow destructive operations on files if subsequent tasks read from those files.
    """
    # Identify destructive tasks (write_file, replace_in_file) and what files they modify
    destructive_tasks = []  # List of (task_idx, file_path, task_description)

    for idx, task in enumerate(plan.tasks):
        # Tasks that modify files are destructive
        if any(word in task.description.lower() for word in ["extract", "refactor", "remove", "delete", "modify"]):
            # Tasks mentioning file operations are potentially destructive
            # Extract likely files from description - generic pattern for any path-like string ending in .py
            mentioned_files = re.findall(r'(?:[a-zA-Z0-9_./\-]+)\.py', task.description)
            if mentioned_files:
                destructive_tasks.append((idx, mentioned_files, task.description))

    if not destructive_tasks:
        # No destructive tasks, so no interdependency issues
        return ValidationResult(
            name="destructive_interdependency_check",
            status=ValidationStatus.PASSED,
            message="No destructive file operations detected"
        )

    # Check if any subsequent task reads from files modified by earlier destructive tasks
    issues = []

    for destructive_idx, modified_files, destructive_desc in destructive_tasks:
        # Check all tasks AFTER this destructive task
        for subsequent_idx in range(destructive_idx + 1, len(plan.tasks)):
            subsequent_task = plan.tasks[subsequent_idx]
            subsequent_desc = subsequent_task.description.lower()

            # Check if subsequent task tries to read from the modified files
            for modified_file in modified_files:
                # If the subsequent task mentions the same file or trying to extract from it
                if modified_file in subsequent_desc:
                    # If it's trying to extract/read from the same file
                    if any(word in subsequent_desc for word in ["extract", "read", "from", "in"]):
                        issues.append({
                            "destructive_task": destructive_desc,
                            "destructive_file": modified_file,
                            "dependent_task": subsequent_task.description,
                            "line": f"Task {destructive_idx + 1} modifies '{modified_file}' but Task {subsequent_idx + 1} tries to read from it"
                        })

    if issues:
        return ValidationResult(
            name="destructive_interdependency_check",
            status=ValidationStatus.FAILED,
            message=f"CRITICAL: {len(issues)} destructive operation(s) break subsequent task dependencies",
            details={
                "issues": issues,
                "recommendation": "Either: (1) Reorder tasks so all reads happen before writes, or (2) Use COPY instead of EXTRACT (don't truncate source file)"
            }
        )

    return ValidationResult(
        name="destructive_interdependency_check",
        status=ValidationStatus.PASSED,
        message=f"Checked {len(destructive_tasks)} destructive task(s) - no dependency violations found"
    )


def quick_validate(plan: ExecutionPlan) -> bool:
    """Quick validation - just check if tasks completed and run tests.

    Args:
        plan: The executed plan

    Returns:
        True if validation passed, False otherwise
    """
    # Check task completion
    failed = [t for t in plan.tasks if t.status == TaskStatus.FAILED]
    if failed:
        return False

    # Quick test run
    try:
        common_dirs = _get_common_dirs()
        test_dir = common_dirs["tests"][0]
        result = execute_tool("run_tests", {"test_path": f"{test_dir}/", "verbose": False})
        result_data = json.loads(result)
        return result_data.get("rc", 1) == 0
    except:
        # If tests cannot run (syntax error, missing dependencies, etc), assume INVALID
        return False
